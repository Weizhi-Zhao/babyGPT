model: configs/models/babyGPT_rope.yaml
optimizer: configs/optimizers/default_optimizer.yaml
dataset: configs/datasets/shakespeare_char_set.yaml
max_new_tokens: 500
condition_prompt: '

  '
temperature: 0.8
top_k: 200
seed: 321
n_layer: 6
n_head: 6
n_embd: 384
dropout: 0.2
block_size: 256
bias: false
rope_base: 1000
out_dir: output/shakespeare_char/RoPE_i_10
eval_interval: 100
eval_num_samples: 10
final_eval_samples: 5000
max_iters: 3000
batch_size: 64
learning_rate: 0.001
beta1: 0.9
beta2: 0.99
weight_decay: 0.1
decay_lr:
  warmup_iters: 100
  lr_decay_iters: 3000
  min_lr: 0.0001
dataset_name: shakespeare_char
train_set_path: data/shakespeare_char/train.pt
test_set_path: data/shakespeare_char/test.pt
meta_path: data/shakespeare_char/meta.pkl
vocab_size: 65
